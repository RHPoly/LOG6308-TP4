{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - BPR Loss: 0.6932 | Best: inf (epoch -1)\n",
      "Epoch 5 - BPR Loss: 0.6931 | Best: 0.6932 (epoch 0)\n",
      "Epoch 10 - BPR Loss: 0.6931 | Best: 0.6932 (epoch 0)\n",
      "Epoch 15 - BPR Loss: 0.6930 | Best: 0.6930 (epoch 13)\n",
      "Epoch 20 - BPR Loss: 0.6928 | Best: 0.6929 (epoch 17)\n",
      "Epoch 25 - BPR Loss: 0.6925 | Best: 0.6925 (epoch 24)\n",
      "Epoch 30 - BPR Loss: 0.6920 | Best: 0.6921 (epoch 29)\n",
      "Epoch 35 - BPR Loss: 0.6914 | Best: 0.6915 (epoch 34)\n",
      "Epoch 40 - BPR Loss: 0.6904 | Best: 0.6906 (epoch 39)\n",
      "Epoch 45 - BPR Loss: 0.6891 | Best: 0.6896 (epoch 44)\n",
      "Epoch 50 - BPR Loss: 0.6879 | Best: 0.6880 (epoch 49)\n",
      "Epoch 55 - BPR Loss: 0.6863 | Best: 0.6864 (epoch 54)\n",
      "Epoch 60 - BPR Loss: 0.6843 | Best: 0.6844 (epoch 59)\n",
      "Epoch 65 - BPR Loss: 0.6820 | Best: 0.6824 (epoch 64)\n",
      "Epoch 70 - BPR Loss: 0.6793 | Best: 0.6800 (epoch 69)\n",
      "Epoch 75 - BPR Loss: 0.6765 | Best: 0.6769 (epoch 74)\n",
      "Epoch 80 - BPR Loss: 0.6735 | Best: 0.6738 (epoch 79)\n",
      "Epoch 85 - BPR Loss: 0.6691 | Best: 0.6709 (epoch 84)\n",
      "Epoch 90 - BPR Loss: 0.6671 | Best: 0.6666 (epoch 89)\n",
      "Epoch 95 - BPR Loss: 0.6632 | Best: 0.6631 (epoch 94)\n",
      "Epoch 100 - BPR Loss: 0.6580 | Best: 0.6591 (epoch 99)\n",
      "Epoch 105 - BPR Loss: 0.6541 | Best: 0.6550 (epoch 104)\n",
      "Epoch 110 - BPR Loss: 0.6505 | Best: 0.6512 (epoch 109)\n",
      "Epoch 115 - BPR Loss: 0.6445 | Best: 0.6458 (epoch 114)\n",
      "Epoch 120 - BPR Loss: 0.6410 | Best: 0.6420 (epoch 119)\n",
      "Epoch 125 - BPR Loss: 0.6368 | Best: 0.6372 (epoch 124)\n",
      "Epoch 130 - BPR Loss: 0.6308 | Best: 0.6299 (epoch 129)\n",
      "Epoch 135 - BPR Loss: 0.6263 | Best: 0.6275 (epoch 134)\n",
      "Epoch 140 - BPR Loss: 0.6209 | Best: 0.6225 (epoch 139)\n",
      "Epoch 145 - BPR Loss: 0.6180 | Best: 0.6172 (epoch 143)\n",
      "Epoch 150 - BPR Loss: 0.6130 | Best: 0.6134 (epoch 147)\n",
      "Epoch 155 - BPR Loss: 0.6062 | Best: 0.6092 (epoch 154)\n",
      "Epoch 160 - BPR Loss: 0.6058 | Best: 0.6026 (epoch 156)\n",
      "Epoch 165 - BPR Loss: 0.5989 | Best: 0.5981 (epoch 164)\n",
      "Epoch 170 - BPR Loss: 0.5917 | Best: 0.5946 (epoch 168)\n",
      "Epoch 175 - BPR Loss: 0.5898 | Best: 0.5873 (epoch 173)\n",
      "Epoch 180 - BPR Loss: 0.5825 | Best: 0.5866 (epoch 179)\n",
      "Epoch 185 - BPR Loss: 0.5748 | Best: 0.5776 (epoch 184)\n",
      "Epoch 190 - BPR Loss: 0.5731 | Best: 0.5748 (epoch 185)\n",
      "Epoch 195 - BPR Loss: 0.5691 | Best: 0.5705 (epoch 192)\n",
      "Epoch 200 - BPR Loss: 0.5668 | Best: 0.5678 (epoch 198)\n",
      "Epoch 205 - BPR Loss: 0.5592 | Best: 0.5615 (epoch 204)\n",
      "Epoch 210 - BPR Loss: 0.5547 | Best: 0.5583 (epoch 209)\n",
      "Epoch 215 - BPR Loss: 0.5528 | Best: 0.5518 (epoch 214)\n",
      "Epoch 220 - BPR Loss: 0.5481 | Best: 0.5488 (epoch 217)\n",
      "Epoch 225 - BPR Loss: 0.5416 | Best: 0.5453 (epoch 223)\n",
      "Epoch 230 - BPR Loss: 0.5404 | Best: 0.5416 (epoch 225)\n",
      "Epoch 235 - BPR Loss: 0.5322 | Best: 0.5353 (epoch 234)\n",
      "Epoch 240 - BPR Loss: 0.5305 | Best: 0.5322 (epoch 235)\n",
      "Epoch 245 - BPR Loss: 0.5294 | Best: 0.5295 (epoch 243)\n",
      "Epoch 250 - BPR Loss: 0.5241 | Best: 0.5227 (epoch 248)\n",
      "Epoch 255 - BPR Loss: 0.5218 | Best: 0.5166 (epoch 253)\n",
      "Epoch 260 - BPR Loss: 0.5183 | Best: 0.5147 (epoch 259)\n",
      "Epoch 265 - BPR Loss: 0.5155 | Best: 0.5147 (epoch 259)\n",
      "Epoch 270 - BPR Loss: 0.5119 | Best: 0.5087 (epoch 269)\n",
      "Epoch 275 - BPR Loss: 0.5124 | Best: 0.5076 (epoch 274)\n",
      "Epoch 280 - BPR Loss: 0.5088 | Best: 0.5005 (epoch 279)\n",
      "Epoch 285 - BPR Loss: 0.5018 | Best: 0.4998 (epoch 282)\n",
      "Epoch 290 - BPR Loss: 0.5023 | Best: 0.4998 (epoch 282)\n",
      "Epoch 295 - BPR Loss: 0.4934 | Best: 0.4960 (epoch 291)\n",
      "Epoch 300 - BPR Loss: 0.4912 | Best: 0.4929 (epoch 297)\n",
      "Epoch 305 - BPR Loss: 0.4909 | Best: 0.4912 (epoch 300)\n",
      "Epoch 310 - BPR Loss: 0.4893 | Best: 0.4895 (epoch 307)\n",
      "Epoch 315 - BPR Loss: 0.4813 | Best: 0.4864 (epoch 314)\n",
      "Epoch 320 - BPR Loss: 0.4870 | Best: 0.4803 (epoch 319)\n",
      "Epoch 325 - BPR Loss: 0.4801 | Best: 0.4760 (epoch 321)\n",
      "Epoch 330 - BPR Loss: 0.4815 | Best: 0.4760 (epoch 321)\n",
      "Epoch 335 - BPR Loss: 0.4787 | Best: 0.4749 (epoch 334)\n",
      "Epoch 340 - BPR Loss: 0.4710 | Best: 0.4720 (epoch 337)\n",
      "Epoch 345 - BPR Loss: 0.4672 | Best: 0.4701 (epoch 344)\n",
      "Epoch 350 - BPR Loss: 0.4660 | Best: 0.4637 (epoch 349)\n",
      "Epoch 355 - BPR Loss: 0.4633 | Best: 0.4622 (epoch 354)\n",
      "Epoch 360 - BPR Loss: 0.4601 | Best: 0.4562 (epoch 359)\n",
      "Epoch 365 - BPR Loss: 0.4541 | Best: 0.4562 (epoch 359)\n",
      "Epoch 370 - BPR Loss: 0.4562 | Best: 0.4541 (epoch 365)\n",
      "Epoch 375 - BPR Loss: 0.4572 | Best: 0.4541 (epoch 365)\n",
      "Epoch 380 - BPR Loss: 0.4535 | Best: 0.4539 (epoch 379)\n",
      "Epoch 385 - BPR Loss: 0.4508 | Best: 0.4520 (epoch 384)\n",
      "Epoch 390 - BPR Loss: 0.4464 | Best: 0.4460 (epoch 389)\n",
      "Epoch 395 - BPR Loss: 0.4496 | Best: 0.4460 (epoch 389)\n",
      "Epoch 400 - BPR Loss: 0.4480 | Best: 0.4412 (epoch 397)\n",
      "Epoch 405 - BPR Loss: 0.4519 | Best: 0.4412 (epoch 397)\n",
      "Epoch 410 - BPR Loss: 0.4448 | Best: 0.4381 (epoch 406)\n",
      "Epoch 415 - BPR Loss: 0.4413 | Best: 0.4360 (epoch 413)\n",
      "Epoch 420 - BPR Loss: 0.4302 | Best: 0.4354 (epoch 419)\n",
      "Epoch 425 - BPR Loss: 0.4346 | Best: 0.4302 (epoch 420)\n",
      "Epoch 430 - BPR Loss: 0.4377 | Best: 0.4302 (epoch 420)\n",
      "Epoch 435 - BPR Loss: 0.4323 | Best: 0.4299 (epoch 434)\n",
      "Epoch 440 - BPR Loss: 0.4297 | Best: 0.4240 (epoch 436)\n",
      "Epoch 445 - BPR Loss: 0.4300 | Best: 0.4240 (epoch 436)\n",
      "Epoch 450 - BPR Loss: 0.4202 | Best: 0.4240 (epoch 436)\n",
      "Epoch 455 - BPR Loss: 0.4269 | Best: 0.4202 (epoch 450)\n",
      "Epoch 460 - BPR Loss: 0.4238 | Best: 0.4195 (epoch 457)\n",
      "Epoch 465 - BPR Loss: 0.4220 | Best: 0.4171 (epoch 464)\n",
      "Epoch 470 - BPR Loss: 0.4254 | Best: 0.4155 (epoch 467)\n",
      "Epoch 475 - BPR Loss: 0.4134 | Best: 0.4096 (epoch 472)\n",
      "Epoch 480 - BPR Loss: 0.4208 | Best: 0.4096 (epoch 472)\n",
      "Epoch 485 - BPR Loss: 0.4165 | Best: 0.4096 (epoch 472)\n",
      "Epoch 490 - BPR Loss: 0.4181 | Best: 0.4096 (epoch 472)\n",
      "Epoch 495 - BPR Loss: 0.4116 | Best: 0.4096 (epoch 472)\n",
      "Epoch 500 - BPR Loss: 0.4141 | Best: 0.4088 (epoch 496)\n",
      "Epoch 505 - BPR Loss: 0.4142 | Best: 0.4072 (epoch 503)\n",
      "Epoch 510 - BPR Loss: 0.4148 | Best: 0.4039 (epoch 508)\n",
      "Epoch 515 - BPR Loss: 0.4103 | Best: 0.3984 (epoch 513)\n",
      "Epoch 520 - BPR Loss: 0.4049 | Best: 0.3984 (epoch 513)\n",
      "Epoch 525 - BPR Loss: 0.4026 | Best: 0.3984 (epoch 513)\n",
      "Epoch 530 - BPR Loss: 0.4010 | Best: 0.3984 (epoch 513)\n",
      "Epoch 535 - BPR Loss: 0.4020 | Best: 0.3984 (epoch 513)\n",
      "Epoch 540 - BPR Loss: 0.4031 | Best: 0.3969 (epoch 536)\n",
      "Epoch 545 - BPR Loss: 0.4031 | Best: 0.3931 (epoch 544)\n",
      "Epoch 550 - BPR Loss: 0.3900 | Best: 0.3916 (epoch 548)\n",
      "Epoch 555 - BPR Loss: 0.3959 | Best: 0.3900 (epoch 550)\n",
      "Epoch 560 - BPR Loss: 0.3972 | Best: 0.3855 (epoch 558)\n",
      "Epoch 565 - BPR Loss: 0.3920 | Best: 0.3839 (epoch 562)\n",
      "Epoch 570 - BPR Loss: 0.3888 | Best: 0.3839 (epoch 562)\n",
      "Epoch 575 - BPR Loss: 0.3932 | Best: 0.3839 (epoch 562)\n",
      "Epoch 580 - BPR Loss: 0.3934 | Best: 0.3839 (epoch 562)\n",
      "Epoch 585 - BPR Loss: 0.3852 | Best: 0.3831 (epoch 582)\n",
      "Epoch 590 - BPR Loss: 0.3920 | Best: 0.3817 (epoch 587)\n",
      "Epoch 595 - BPR Loss: 0.3845 | Best: 0.3799 (epoch 591)\n",
      "Epoch 600 - BPR Loss: 0.3878 | Best: 0.3799 (epoch 591)\n",
      "Epoch 605 - BPR Loss: 0.3853 | Best: 0.3777 (epoch 603)\n",
      "Epoch 610 - BPR Loss: 0.3843 | Best: 0.3777 (epoch 603)\n",
      "Epoch 615 - BPR Loss: 0.3858 | Best: 0.3777 (epoch 603)\n",
      "Epoch 620 - BPR Loss: 0.3765 | Best: 0.3763 (epoch 616)\n",
      "Epoch 625 - BPR Loss: 0.3827 | Best: 0.3729 (epoch 624)\n",
      "Epoch 630 - BPR Loss: 0.3723 | Best: 0.3729 (epoch 624)\n",
      "Epoch 635 - BPR Loss: 0.3706 | Best: 0.3723 (epoch 630)\n",
      "Epoch 640 - BPR Loss: 0.3720 | Best: 0.3706 (epoch 635)\n",
      "Epoch 645 - BPR Loss: 0.3767 | Best: 0.3653 (epoch 641)\n",
      "Epoch 650 - BPR Loss: 0.3713 | Best: 0.3653 (epoch 641)\n",
      "Epoch 655 - BPR Loss: 0.3722 | Best: 0.3653 (epoch 641)\n",
      "Epoch 660 - BPR Loss: 0.3718 | Best: 0.3645 (epoch 656)\n",
      "Epoch 665 - BPR Loss: 0.3715 | Best: 0.3645 (epoch 656)\n",
      "Epoch 670 - BPR Loss: 0.3707 | Best: 0.3645 (epoch 656)\n",
      "Epoch 675 - BPR Loss: 0.3683 | Best: 0.3645 (epoch 656)\n",
      "Epoch 680 - BPR Loss: 0.3679 | Best: 0.3634 (epoch 677)\n",
      "Epoch 685 - BPR Loss: 0.3761 | Best: 0.3614 (epoch 681)\n",
      "Epoch 690 - BPR Loss: 0.3629 | Best: 0.3601 (epoch 687)\n",
      "Epoch 695 - BPR Loss: 0.3654 | Best: 0.3578 (epoch 692)\n",
      "Epoch 700 - BPR Loss: 0.3608 | Best: 0.3578 (epoch 692)\n",
      "Epoch 705 - BPR Loss: 0.3580 | Best: 0.3578 (epoch 692)\n",
      "Epoch 710 - BPR Loss: 0.3581 | Best: 0.3578 (epoch 692)\n",
      "Epoch 715 - BPR Loss: 0.3569 | Best: 0.3544 (epoch 713)\n",
      "Epoch 720 - BPR Loss: 0.3551 | Best: 0.3544 (epoch 713)\n",
      "Epoch 725 - BPR Loss: 0.3611 | Best: 0.3544 (epoch 713)\n",
      "Epoch 730 - BPR Loss: 0.3521 | Best: 0.3537 (epoch 726)\n",
      "Epoch 735 - BPR Loss: 0.3579 | Best: 0.3521 (epoch 730)\n",
      "Epoch 740 - BPR Loss: 0.3544 | Best: 0.3465 (epoch 737)\n",
      "Epoch 745 - BPR Loss: 0.3558 | Best: 0.3465 (epoch 737)\n",
      "Epoch 750 - BPR Loss: 0.3539 | Best: 0.3465 (epoch 737)\n",
      "Epoch 755 - BPR Loss: 0.3501 | Best: 0.3465 (epoch 737)\n",
      "Epoch 760 - BPR Loss: 0.3539 | Best: 0.3465 (epoch 737)\n",
      "Epoch 765 - BPR Loss: 0.3471 | Best: 0.3465 (epoch 737)\n",
      "Early stopping at epoch 767 (no improvement for 30 epochs)\n",
      "Approximate Recall@20 on direct neighbors: 0.1120\n",
      "Article 56d87c8bdabfae2eee45fce6 => ['558bf6dc0cf20e727d0f4b1c', '53e9aab0b7602d970342da1d', '53e9b403b7602d9703ef5d92', '53e9bb23b7602d970475d3f4', '53e9ab37b7602d97034c4455', '558b3daa84ae84d265c246c1', '53e9bb6cb7602d97047ae537', '53e9aba4b7602d970354e033', '558c197f84ae6766fdf10bdc', '53e9bcc5b7602d97049479c2', '53e9a06db7602d9702953252', '53e9b2ccb7602d9703d794b0', '53e9aa09b7602d9703377fec', '53e9b254b7602d9703cf75f5', '53e9ab48b7602d97034dcba5', '53e99ce0b7602d9702597c76', '53e9aa10b7602d970337f51e', '53e99ba2b7602d9702448634', '53e9ab69b7602d970350aaa0', '53e9aebcb7602d97038e4188']\n",
      "Article 53e9b7d9b7602d9704389448 => ['56d81390dabfae2eee62543c', '53e9b2f9b7602d9703db802e', '53e9b60db7602d9704160535', '53e9b857b7602d970441c4d8', '53e9ae22b7602d9703835237', '53e9b75ab7602d97042fd5af', '53e9ba9ab7602d97046c21dc', '53e99ae2b7602d970236838a', '53e9bb86b7602d97047ce8c6', '53e9a42cb7602d9702d4a5db', '53e9b153b7602d9703bd99ed', '53e9a11cb7602d9702a0e652', '53e9ba60b7602d970467a2bb', '53e9b2aab7602d9703d5221f', '557f18916fee0fe990caebcf', '53e9ab55b7602d97034e6fac', '558a9da7e4b0b32fcb37d87f', '53e99e6ab7602d970272ee1b', '53e9bd4bb7602d97049d8f41', '53e99c04b7602d97024b1d56']\n",
      "Article 53e99e04b7602d97026ca90e => ['558bef2ce4b00c3c48df5012', '558b6b40e4b031bae1fd4a25', '53e9b62db7602d9704184ddc', '53e9a58bb7602d9702eb2214', '53e99b94b7602d97024397ac', '53e99b1bb7602d97023b2c7a', '53e9b708b7602d970429edd1', '53e9a952b7602d97032a720b', '53e9b36cb7602d9703e4d59b', '53e9b69eb7602d9704215e5a', '53e9a327b7602d9702c331db', '53e9b61bb7602d970416fb52', '53e9b7adb7602d9704353e29', '53e9b3efb7602d9703ee234d', '53e99fd0b7602d97028aa89d', '53e9b7bab7602d97043632d1', '53e9b6cbb7602d9704259258', '53e9b3c1b7602d9703eabb1c', '558c90b2e4b0cfb70a1e66a0', '53e9bc32b7602d97048a2da4']\n",
      "Article 53e9b32bb7602d9703dfdacf => ['53e9a97bb7602d97032d37a4', '53e99b43b7602d97023e3915', '53e9b565b7602d97040a130e', '53e9a2f3b7602d9702bfda11', '53e9b61bb7602d9704173601', '53e9982cb7602d970204e432', '53e9b4c3b7602d9703fde08c', '53e99aacb7602d97023267eb', '53e9b30ab7602d9703dcdc89', '53e9b790b7602d9704335ccb', '53e9b268b7602d9703d0d3dd', '53e9a238b7602d9702b3f2cf', '53e99a9fb7602d970231797f', '53e9a833b7602d970317db69', '53e99db1b7602d970266d400', '53e9b19bb7602d9703c24306', '53e99d2fb7602d97025e5be7', '53e9b9fbb7602d97045f75db', '53e9b97db7602d970456fb39', '53e9a727b7602d970305bbaa']\n",
      "Article 53e9b9adb7602d970459ef8c => ['53e99b63b7602d9702406def', '53e9a480b7602d9702d9b7d1', '53e9a562b7602d9702e875b8', '53e9a26ab7602d9702b71511', '53e9be57b7602d9704b19652', '53e9ab73b7602d97035195f0', '53e9bde9b7602d9704a9c590', '53e9b6cbb7602d9704255ef1', '53e9b923b7602d970450adc9', '53e9a289b7602d9702b8f511', '53e9be27b7602d9704ae159d', '53e99ab2b7602d970232c88f', '53e9acaeb7602d970368f2da', '53e9a472b7602d9702d8e7c9', '53e9acdab7602d97036b8659', '53e9bbd5b7602d97048279da', '53e99df7b7602d97026ba544', '53e9a281b7602d9702b8881c', '53e9a9a9b7602d9703306836', '53e9b9a6b7602d9704597c14']\n",
      "Vérification des recommandations...\n",
      "100 articles avec 20 recommandations valides\n",
      "Tout est prêt pour l'export\n",
      "Fichier de soumission 'soumission_lightgcn.csv' généré avec succès.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.io import mmread\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------- 1. Normalisation de la matrice d'adjacence -------------------------\n",
    "def normalize_adj(adj):\n",
    "    rowsum = np.array(adj.sum(1)).flatten()\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5, where=rowsum != 0)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0. # \"Failsafe\"\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return d_mat_inv_sqrt @ adj @ d_mat_inv_sqrt\n",
    "\n",
    "# ------------------------- 2. Définition du modèle LightGCN -------------------------\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim=64, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.num_layers = num_layers\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, norm_adj):\n",
    "        x = self.embedding.weight\n",
    "        all_embeddings = [x]\n",
    "        for _ in range(self.num_layers):\n",
    "            x = torch.sparse.mm(norm_adj, x)\n",
    "            all_embeddings.append(x)\n",
    "        return torch.stack(all_embeddings, dim=0).mean(dim=0)  # Moyenne des couches\n",
    "\n",
    "    def recommend(self, embeddings, node_ids, top_k=20):\n",
    "        norm_embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "        selected = norm_embeddings[node_ids]\n",
    "        scores = selected @ norm_embeddings.T\n",
    "        scores[torch.arange(len(node_ids)).unsqueeze(1), node_ids.unsqueeze(1)] = -1e9  # Exclure soi-même\n",
    "        return torch.topk(scores, top_k, dim=1).indices\n",
    "\n",
    "# ------------------------- 3. Chargement des données -------------------------\n",
    "adj_direct = mmread(\"data/TP4-matrice-adjacence.dgt\").tocsr()\n",
    "# Ajout de la co-citation\n",
    "co_citation = adj_direct.T @ adj_direct\n",
    "co_citation.setdiag(0)\n",
    "co_citation.eliminate_zeros()\n",
    "threshold = 2 # Seuil minimal pour réduire le bruit\n",
    "co_citation.data[co_citation.data < threshold] = 0\n",
    "co_citation.eliminate_zeros()\n",
    "co_citation.data = np.log1p(co_citation.data)  # Pondération logarithmique\n",
    "alpha = 0.8  # poids du graphe direct\n",
    "adj = alpha * adj_direct + (1 - alpha) * co_citation\n",
    "\n",
    "ids_df = pd.read_csv(\"data/TP4-ids.csv\", dtype=str)\n",
    "ids = ids_df[\"id\"].values\n",
    "ids_test_df = pd.read_csv(\"data/TP4-ids-test.csv\", dtype=str).dropna()\n",
    "ids_test = ids_test_df[\"id\"].values\n",
    "# Mapping entre identifiants et indices dans la matrice\n",
    "id_to_index = {id_: idx for idx, id_ in enumerate(ids)}\n",
    "test_indices = [id_to_index[x] for x in ids_test]\n",
    "\n",
    "# ------------------------- 4. Préparation de la matrice normalisée -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "norm_adj = normalize_adj(adj + adj.T).tocoo()  # symétriser le graphe\n",
    "norm_adj_indices = torch.LongTensor(np.vstack((norm_adj.row, norm_adj.col)))\n",
    "norm_adj_values = torch.FloatTensor(norm_adj.data)\n",
    "norm_adj_tensor = torch.sparse_coo_tensor(norm_adj_indices, norm_adj_values, torch.Size(norm_adj.shape)).to(device)\n",
    "\n",
    "# ------------------------- 5. Entraînement avec BPR Loss -------------------------\n",
    "model = LightGCN(num_nodes=adj.shape[0]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def bpr_loss(u_emb, i_emb, j_emb):\n",
    "    score_pos = (u_emb * i_emb).sum(dim=1)\n",
    "    score_neg = (u_emb * j_emb).sum(dim=1)\n",
    "    return -torch.log(torch.sigmoid(score_pos - score_neg)).mean()\n",
    "\n",
    "def evaluate_and_log(epoch, loss, best_loss, best_epoch, epochs_no_improve):\n",
    "    print(f\"Epoch {epoch} - BPR Loss: {loss.item():.4f} | Best: {best_loss:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "edge_index = np.vstack((adj.nonzero()[0], adj.nonzero()[1])).T\n",
    "num_nodes = adj.shape[0]\n",
    "batch_size = 8192\n",
    "epochs = 800\n",
    "k = 50  # Nombre de candidats négatifs\n",
    "\n",
    "# Params early stopping\n",
    "best_loss = float('inf')\n",
    "best_epoch = -1\n",
    "epochs_no_improve = 0\n",
    "patience = 30\n",
    "min_delta = 1e-4\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    emb = model(norm_adj_tensor)\n",
    "\n",
    "    # Echantillonnage positifs et négatifs\n",
    "    sampled_idx = np.random.randint(0, len(edge_index), size=batch_size)\n",
    "    u_np, i_np = edge_index[sampled_idx].T\n",
    "    neg_candidates = np.random.randint(0, num_nodes, size=(batch_size, k))\n",
    "\n",
    "    i_np_broadcasted = np.repeat(i_np[:, np.newaxis], k, axis=1)\n",
    "    invalid_j_eq_i = (neg_candidates == i_np_broadcasted)\n",
    "    adj_csr = adj.tocsr()\n",
    "\n",
    "    rows = np.repeat(u_np, k)\n",
    "    cols = neg_candidates.flatten()\n",
    "    is_edge = np.array(adj_csr[rows, cols] != 0).reshape(batch_size, k)\n",
    "    mask_valid = ~(invalid_j_eq_i | is_edge)\n",
    "\n",
    "    # Conversion en tensors\n",
    "    u_tensor = torch.tensor(u_np, device=device)\n",
    "    i_tensor = torch.tensor(i_np, device=device)\n",
    "    j_tensor_all = torch.tensor(neg_candidates, device=device)\n",
    "    mask_valid_tensor = torch.tensor(mask_valid, device=device)\n",
    "\n",
    "    # Sélection des meilleurs j\n",
    "    u_emb = emb[u_tensor]\n",
    "    i_emb = emb[i_tensor]\n",
    "    j_emb_all = emb[j_tensor_all]\n",
    "    u_emb_exp = u_emb.unsqueeze(1)\n",
    "    scores = (u_emb_exp * j_emb_all).sum(dim=2)\n",
    "    scores[~mask_valid_tensor] = -1e9\n",
    "    best_j_indices = scores.argmax(dim=1)\n",
    "    j_tensor = j_tensor_all[torch.arange(batch_size), best_j_indices]\n",
    "    j_emb = emb[j_tensor]\n",
    "\n",
    "    # Optimisation\n",
    "    loss = bpr_loss(u_emb, i_emb, j_emb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        evaluate_and_log(epoch, loss, best_loss, best_epoch, epochs_no_improve)\n",
    "\n",
    "    if best_loss - loss.item() > min_delta:\n",
    "        best_loss = loss.item()\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "# ------------------------- 6. Génération des recommandations -------------------------\n",
    "def generate_recommendations(model, norm_adj_tensor, test_indices, ids):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_embeddings = model(norm_adj_tensor)\n",
    "        test_tensor = torch.tensor(test_indices).to(device)\n",
    "        topk = model.recommend(final_embeddings, test_tensor, top_k=20)\n",
    "        \n",
    "    results = {}\n",
    "    for i, test_idx in enumerate(test_indices):\n",
    "        recommended = topk[i].cpu().numpy()\n",
    "        recommended_ids = [str(ids[x]) for x in recommended]\n",
    "        results[str(ids[test_idx])] = recommended_ids\n",
    "    return results\n",
    "\n",
    "reco_results = generate_recommendations(model, norm_adj_tensor.to(device), test_indices, ids)\n",
    "\n",
    "def recall_at_k(reco_results, adj, test_indices, ids, k=20):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx in test_indices:\n",
    "        true_neighbors = set(adj[idx].indices)\n",
    "        recommended = set(id_to_index.get(x, -1) for x in reco_results[ids[idx]][:k])\n",
    "        recommended.discard(-1)\n",
    "        hits += len(true_neighbors & recommended)\n",
    "        total += len(true_neighbors)\n",
    "\n",
    "    return hits / total if total > 0 else 0.0\n",
    "\n",
    "recall_score = recall_at_k(reco_results, adj, test_indices, ids, k=20)\n",
    "print(f\"Approximate Recall@20 on direct neighbors: {recall_score:.4f}\")\n",
    "\n",
    "# ------------------------- 7. Affichage des résultats -------------------------\n",
    "for test_article, recs in list(reco_results.items())[:5]:\n",
    "    print(f\"Article {test_article} => {recs}\")\n",
    "\n",
    "# ------------------------- 8. Sauvegarde des recommandations -------------------------\n",
    "# Vérification des résultats avant écriture\n",
    "print(\"Vérification des recommandations...\")\n",
    "\n",
    "valid_ids = set(ids)\n",
    "nb_valid = 0\n",
    "nb_invalid = 0\n",
    "nb_wrong_length = 0\n",
    "\n",
    "for article_id, recs in reco_results.items():\n",
    "    if len(recs) != 20:\n",
    "        print(f\"WARNING: Article {article_id} : {len(recs)} recommandations (au lieu de 20)\")\n",
    "        nb_wrong_length += 1\n",
    "    invalids = [rec for rec in recs if rec not in valid_ids]\n",
    "    if invalids:\n",
    "        print(f\"WARNING: Article {article_id} : {len(invalids)} identifiants invalides : {invalids}\")\n",
    "        nb_invalid += 1\n",
    "    else:\n",
    "        nb_valid += 1\n",
    "\n",
    "print(f\"{nb_valid} articles avec 20 recommandations valides\")\n",
    "if nb_wrong_length or nb_invalid:\n",
    "    print(f\"WARNING: {nb_wrong_length} avec mauvais nombre de recommandations, {nb_invalid} avec ID invalides\")\n",
    "else:\n",
    "    print(\"Tout est prêt pour l'export\")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# Ecriture dans un fichier CSV au format requis\n",
    "with open(\"soumission_lightgcn.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"recommandations\"])  # en-tête\n",
    "    for article_id, recs in reco_results.items():\n",
    "        writer.writerow([article_id, \" \".join(recs)])\n",
    "\n",
    "print(\"Fichier de soumission 'soumission_lightgcn.csv' généré avec succès.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normaliser les embeddings\n",
    "- Dropout embeddings\n",
    "- Scheduler de LR\n",
    "- Could change embedding dim to 128 or 256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
